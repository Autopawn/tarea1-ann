{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Predicción de Entalpía de Atomización\n",
    "\n",
    "\n",
    "Las simulaciones de propiedades moleculares son computacionalmente costosas y requieren de un arduo trabajo científico. El objetivo de esta sección corresponde a la utilización de métodos de aprendizaje automático supervisado (Redes Neuronales Artificiales) para predecir propiedades moleculares, en este caso la Energía de Atomización o Entalpía de Atomización, a partir de una base de datos de simulaciones obtenida mediante __[Quantum Espresso](http://www.quantum-espresso.org/)__. Si esto se lograse hacer con gran precisión, se abrirían muchas posibilidades en el diseño computacional y el descubrimiento de nuevas moléculas, compuestos y fármacos.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/services/images/RSCpubs.ePlatform.Service.FreeContent.ImageService.svc/ImageService/Articleimage/2012/NR/c2nr11543c/c2nr11543c-f4.gif\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "La **entalpía de atomización** es la cantidad de variación de entalpía cuando los enlaces de un compuesto se rompen y los componentes se reducen a átomos individuales. Tal como se ha indicado, su tarea es la de predecir dicho nivel a partir de los atributos enunciados en el dataset puesto a vuestra disposición en *moodle*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NOTA:**\n",
    "Debido a lo mucho que toma realizar los experimentos, los errores se guardan en archivos caché `.npy`, estos deben borrarse si se quiere repetir los experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"roboBohr.csv\")\n",
    "print(\"datos.shape:\",datos.shape)\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total = len(datos)\n",
    "df_trai = datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_vali = datos[int(0.6*total):int(0.85*total)]        #25% de los datos\n",
    "df_test = datos[int(0.85*total)::]                     #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `Unnamed: 0` representa el id del compuesto dentro del dataset, y `pubchem_id` parece ser un id general para identificar el compuesto. Ambas columnas se remueven porque la asignación de estos índices es arbitraria y su valor no debería estar relacionado con el resultado que debe entregar nuestro modelo (la Entalpía de Atomización, correspondiente a la columna `Eat`). Aunque el modelo debería detectar que no hay correlación entre `Eat` y estos atributos, es mejor removerlos para no *confundir* el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "# Get scaler and scale data\n",
    "scaler = StandardScaler().fit(df_trai)\n",
    "X_trai_scaled = pd.DataFrame(scaler.transform(df_trai),columns=df_trai.columns)\n",
    "X_vali_scaled = pd.DataFrame(scaler.transform(df_vali),columns=df_vali.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "# Get targets\n",
    "y_trai = df_trai.pop('Eat').values.reshape(-1,1)\n",
    "y_vali = df_vali.pop('Eat').values.reshape(-1,1)\n",
    "y_test = df_test.pop('Eat').values.reshape(-1,1)\n",
    "# Remove targets from attributes\n",
    "X_trai_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_vali_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos modelos de aprendizaje son suceptibles a la escala de los atributos, por ese motivo, atributos con órdenes de magnitud mayores pueden afectar más a los mismos.\n",
    "Para hacer el aprendizaje independiente de las unidades de medición en que se presentan estos atributos y evitar un bias del aprendizaje hacia unos por sobre otros, es que se normalizan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create model:\n",
    "def create_model(activation='sigmoid',initializer='uniform',\n",
    "                 lr=None,decay=None,optimizer=None,\n",
    "                 layer1_reg=None,layer2_reg=None,dropout=None,\n",
    "                 neurons=256):\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons,input_dim=X_trai_scaled.shape[1],\n",
    "                    kernel_initializer=initializer,activation=activation,\n",
    "                    kernel_regularizer=layer1_reg))\n",
    "    if dropout:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, kernel_initializer=initializer,activation=\"linear\",\n",
    "                    kernel_regularizer=layer2_reg))\n",
    "    # Set optimizer\n",
    "    if optimizer is None:\n",
    "        # default values\n",
    "        if lr is None: lr = 0.01\n",
    "        if decay is None: decay = 0.0\n",
    "        optimizer = SGD(lr=lr,decay=decay)\n",
    "    else:\n",
    "        assert lr is None and decay is None\n",
    "    # Compile model with optimizer\n",
    "    model.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_color(i,total):\n",
    "    if i<total//2:\n",
    "        return (2.0*i/float(total),0.0,0.0)\n",
    "    else:\n",
    "        return (1.0,2.0*(i-total//2)/float(total),0.0)\n",
    "\n",
    "class PlotErrors(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,text=None,labels=None,gradual=False,save_name=None):\n",
    "        self.trai_err = []\n",
    "        self.vali_err = []\n",
    "        self.save_name = save_name\n",
    "        self.loaded = False\n",
    "        # Load cache\n",
    "        if os.path.isfile(self.save_name):\n",
    "            data = np.load(self.save_name)\n",
    "            self.trai_err = data[0]\n",
    "            self.vali_err = data[1]\n",
    "            self.loaded = True\n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "        self.gradual = gradual\n",
    "        self.trai_title = \"Training error v/s epoch\"\n",
    "        self.vali_title = \"Validaton error v/s epoch\"\n",
    "        if text:\n",
    "            self.trai_title = self.trai_title+\" for different \"+text\n",
    "            self.vali_title = self.vali_title+\" for different \"+text\n",
    "        self.widget = interact(self.display).widget\n",
    "\n",
    "    def display(self):\n",
    "        fig, ax = plt.subplots(1,2,figsize=(12,6),sharex=True,sharey=True)\n",
    "        ax[0].set_ylim(0.01,1000)\n",
    "        # Legend and colors:\n",
    "        legc = {}\n",
    "        # Train error:\n",
    "        ax[0].set(xlabel='epoch',ylabel='training error',title=self.trai_title)\n",
    "        for i in range(len(self.trai_err)):\n",
    "            #\n",
    "            if self.labels and self.gradual: legc['c'] = gradual_color(i,len(self.labels))\n",
    "            if self.labels: legc['label'] = self.labels[i]\n",
    "            #\n",
    "            x = np.arange(1,1+len(self.trai_err[i]))\n",
    "            ax[0].semilogy(x,self.trai_err[i],linewidth=2,**legc)\n",
    "        ax[0].grid()\n",
    "        if self.labels: ax[0].legend()\n",
    "        # Validation error\n",
    "        ax[1].set(xlabel='epoch',ylabel='validation error',title=self.vali_title)\n",
    "        for i in range(len(self.vali_err)):\n",
    "            #\n",
    "            if self.labels and self.gradual: legc['c'] = gradual_color(i,len(self.labels))\n",
    "            if self.labels: legc['label'] = self.labels[i]\n",
    "            #\n",
    "            x = np.arange(1,1+len(self.vali_err[i]))\n",
    "            ax[1].semilogy(x,self.vali_err[i],linewidth=2,**legc)\n",
    "        ax[1].grid()\n",
    "        if self.labels: ax[1].legend()\n",
    "        #\n",
    "        plt.show()\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.trai_err.append([])\n",
    "        self.vali_err.append([])\n",
    "    \n",
    "    def on_train_end(self,logs={}):\n",
    "        if self.save_name:\n",
    "            trai_err = np.array(self.trai_err)\n",
    "            u = np.zeros((2,*trai_err.shape))\n",
    "            u[0] = trai_err\n",
    "            u[1] = np.array(self.vali_err)\n",
    "            np.save(self.save_name,u)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        vali_loss = logs.get('val_loss')\n",
    "        trai_loss = logs.get('loss')\n",
    "        self.vali_err[-1].append(vali_loss)\n",
    "        self.trai_err[-1].append(trai_loss)\n",
    "        self.widget.update()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 # Not 250\n",
    "EXPERIMENTS = 5\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callback = PlotErrors(\"experiments\",save_name=\"1b.npy\")\n",
    "if not callback.loaded:\n",
    "    for i in range(EXPERIMENTS):\n",
    "        model = create_model(activation=\"sigmoid\")\n",
    "        model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Respuesta**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "callback = PlotErrors(\"experiments\",save_name=\"1c.npy\")\n",
    "if not callback.loaded:\n",
    "    for i in range(EXPERIMENTS):\n",
    "        initi = keras.initializers.RandomUniform(minval=-1e-5, maxval=1e-5)\n",
    "        model = create_model(activation=\"relu\",initializer=\"zeros\")\n",
    "        model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO Respuesta**\n",
    "\n",
    "Cabe señalar que se debió utilizar un inicializador lineal con valores pequeños, ya que `kernel_initializer=\"uniform\"` resultó en pérdidas `NaN` y `kernel_initializer=\"zeros\"` resulta en experimentos con resultados iguales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lr = 10\n",
    "learn_rate = np.arange(1,n_lr+1)/(20.0*n_lr)\n",
    "print(learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"learning rates\",[str(x) for x in learn_rate],gradual=True,\n",
    "                          save_name=\"1d_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for lr in learn_rate:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",lr=lr) # Notice zeros\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que para learning rates grandes la pérdida se transformó en nan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.02\n",
    "n_decays = 10\n",
    "decays = np.logspace(-6,0,n_decays)\n",
    "print(decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"learning decays\",[str(x) for x in decays],gradual=True,\n",
    "                         save_name=\"1e_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for dec in decays:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",lr=INIT_LR,decay=dec)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con [esta respuesta](https://stats.stackexchange.com/a/211340), la tasa de aprendizaje dado un *decay* $D$ es:\n",
    "$$\n",
    "L_t = L_0 \\, \\frac{1}{1 +  t \\cdot D}\n",
    "$$\n",
    "\n",
    "Se colocó una learning rate relativamente alta pero no lo suficiente como para que resulten pérdidas `nan`.\n",
    "\n",
    "Se requiere realizar varias veces el mismo experimento, comenzar con inicialización zeros no es garantía de que la convergencia de redes al cambiar hiperparámetros será uniforme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 8\n",
    "batch_sizes = np.array(np.round(\n",
    "    np.linspace(100,X_trai_scaled.shape[0]//2,n_batches)),dtype='int')\n",
    "print(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"batch sizes\",[str(x) for x in batch_sizes],gradual=True,\n",
    "                         save_name=\"1f_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for bs in batch_sizes:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\")\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback],\n",
    "                batch_size=bs)\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [SGD,Adam,RMSprop,Adagrad,Adadelta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"optimizers\",[x.__name__ for x in optimizers],\n",
    "                         save_name=\"1g_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for opt in optimizers:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",optimizer=opt(lr=0.01))\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [l1(0.001),l1(0.01),l1(0.1),l2(0.001),l2(0.01),l2(0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"regularizers\",[\"l1=%.3f,l2=%.3f \"%(x.l1,x.l2) for x in regs],\n",
    "                         save_name=\"1h1_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for reg in regs:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",\n",
    "                                 layer1_reg=reg,layer2_reg=reg)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_regs = {\"l1_1layer\":(l1(0.01),None),\n",
    "             \"l1_2layer\":(None,l1(0.01)),\n",
    "             \"l1_both\"  :(l1(0.01),l1(0.01)),\n",
    "             \"l2_1layer\":(l2(0.01),None),\n",
    "             \"l2_2layer\":(None,l2(0.01)),\n",
    "             \"l2_both\"  :(l2(0.01),l2(0.01))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = list(dual_regs.keys())\n",
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"regularizers\",names,\n",
    "                         save_name=\"1h2_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for nam in names:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",\n",
    "                                 layer1_reg=dual_regs[nam][0],layer2_reg=dual_regs[nam][1])\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropouts = [0.2,0.4,0.6,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"dropouts\",[str(x) for x in dropouts],\n",
    "                         save_name=\"1i_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for drop in dropouts:\n",
    "            model = create_model(activation=activ,initializer=\"zeros\",dropout=drop)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                        validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_numbers = [64,128,192,256,320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [],
   "source": [
    "# Create the folds\n",
    "for activation in (\"sigmoid\",\"relu\"):\n",
    "    print(\"Activation %s:\"%activation)\n",
    "    for folds in (5,10):\n",
    "        print(\"\\tFolds %d:\"%folds)\n",
    "        Xm = X_trai_scaled.values\n",
    "        ym = y_trai\n",
    "        for nneurons in neuron_numbers:\n",
    "            kfold = KFold(n_splits=folds,shuffle=False)\n",
    "            cvscores = []\n",
    "            for train, val in kfold.split(X_trai_scaled):\n",
    "                model = create_model(activation=activ,initializer=\"zeros\",neurons=nneurons)\n",
    "                model.fit(Xm[train],ym[train], epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE)\n",
    "                scores = model.evaluate(Xm[val], ym[val], verbose=0)\n",
    "                cvscores.append(scores)\n",
    "            mse_cv = np.mean(cvscores)\n",
    "            mse_test = np.mean(model.evaluate(X_test_scaled,y_test, verbose=0))\n",
    "            print(\"\\t\\t%4d Neurons:  mse_cv: %10.6f  mse_test: %10.6f\"%(\n",
    "                nneurons,mse_cv,mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
