{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Predicción de Entalpía de Atomización\n",
    "\n",
    "\n",
    "Las simulaciones de propiedades moleculares son computacionalmente costosas y requieren de un arduo trabajo científico. El objetivo de esta sección corresponde a la utilización de métodos de aprendizaje automático supervisado (Redes Neuronales Artificiales) para predecir propiedades moleculares, en este caso la Energía de Atomización o Entalpía de Atomización, a partir de una base de datos de simulaciones obtenida mediante __[Quantum Espresso](http://www.quantum-espresso.org/)__. Si esto se lograse hacer con gran precisión, se abrirían muchas posibilidades en el diseño computacional y el descubrimiento de nuevas moléculas, compuestos y fármacos.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/services/images/RSCpubs.ePlatform.Service.FreeContent.ImageService.svc/ImageService/Articleimage/2012/NR/c2nr11543c/c2nr11543c-f4.gif\" title=\"Title text\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "La **entalpía de atomización** es la cantidad de variación de entalpía cuando los enlaces de un compuesto se rompen y los componentes se reducen a átomos individuales. Tal como se ha indicado, su tarea es la de predecir dicho nivel a partir de los atributos enunciados en el dataset puesto a vuestra disposición en *moodle*.\n",
    "\n",
    "**Nota: Debido a lo mucho que toma realizar los experimentos, los errores se guardan en archivos caché `.npy`, estos deben borrarse si se quiere repetir los experimentos.**\n",
    "\n",
    "**SI NO SE VISUALIZAN LOS WIDGETS, CORRA LAS CELDAS DE NUEVO, NO SE ENTRENARÁN REDES**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NOTA:**\n",
    "Debido a lo mucho que toma realizar los experimentos, los errores se guardan en archivos caché `.npy`, estos deben borrarse si se quiere repetir los experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "a) Construya un *dataframe* con los datos a analizar y descríbalo brevemete. Además, realice la división de éste en los conjuntos de entrenamiento, validación y testeo correspondientes. Comente por qué se deben eliminar ciertas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datos.shape: (16242, 1278)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8139.041805</td>\n",
       "      <td>115.715266</td>\n",
       "      <td>22.445723</td>\n",
       "      <td>20.474191</td>\n",
       "      <td>18.529573</td>\n",
       "      <td>17.169350</td>\n",
       "      <td>15.816888</td>\n",
       "      <td>15.133152</td>\n",
       "      <td>14.471534</td>\n",
       "      <td>13.960759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>33107.484300</td>\n",
       "      <td>-11.178969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4698.182820</td>\n",
       "      <td>113.198503</td>\n",
       "      <td>8.659586</td>\n",
       "      <td>7.670481</td>\n",
       "      <td>6.485777</td>\n",
       "      <td>5.512560</td>\n",
       "      <td>4.179691</td>\n",
       "      <td>3.885091</td>\n",
       "      <td>3.503075</td>\n",
       "      <td>3.357136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>23456.785147</td>\n",
       "      <td>3.659133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>2.906146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.245373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4068.250000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.969345</td>\n",
       "      <td>16.228071</td>\n",
       "      <td>15.165862</td>\n",
       "      <td>13.744092</td>\n",
       "      <td>13.653146</td>\n",
       "      <td>13.637784</td>\n",
       "      <td>12.759519</td>\n",
       "      <td>12.587359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.250000</td>\n",
       "      <td>-13.475805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.500000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.662511</td>\n",
       "      <td>18.631287</td>\n",
       "      <td>17.690729</td>\n",
       "      <td>16.020040</td>\n",
       "      <td>15.156646</td>\n",
       "      <td>13.848274</td>\n",
       "      <td>13.659233</td>\n",
       "      <td>13.652832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27731.500000</td>\n",
       "      <td>-10.835211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12207.750000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>21.132432</td>\n",
       "      <td>20.739496</td>\n",
       "      <td>18.712895</td>\n",
       "      <td>18.297501</td>\n",
       "      <td>17.639688</td>\n",
       "      <td>16.154918</td>\n",
       "      <td>15.499474</td>\n",
       "      <td>14.900585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55020.750000</td>\n",
       "      <td>-8.623903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16272.000000</td>\n",
       "      <td>388.023441</td>\n",
       "      <td>73.563510</td>\n",
       "      <td>66.269180</td>\n",
       "      <td>66.268891</td>\n",
       "      <td>66.268756</td>\n",
       "      <td>66.268196</td>\n",
       "      <td>66.264158</td>\n",
       "      <td>66.258487</td>\n",
       "      <td>66.258177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.061999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>74980.000000</td>\n",
       "      <td>-0.789513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             0             1             2             3  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean    8139.041805    115.715266     22.445723     20.474191     18.529573   \n",
       "std     4698.182820    113.198503      8.659586      7.670481      6.485777   \n",
       "min        0.000000     36.858105      2.906146      0.000000      0.000000   \n",
       "25%     4068.250000     73.516695     17.969345     16.228071     15.165862   \n",
       "50%     8142.500000     73.516695     20.662511     18.631287     17.690729   \n",
       "75%    12207.750000     73.516695     21.132432     20.739496     18.712895   \n",
       "max    16272.000000    388.023441     73.563510     66.269180     66.268891   \n",
       "\n",
       "                  4             5             6             7             8  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean      17.169350     15.816888     15.133152     14.471534     13.960759   \n",
       "std        5.512560      4.179691      3.885091      3.503075      3.357136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       13.744092     13.653146     13.637784     12.759519     12.587359   \n",
       "50%       16.020040     15.156646     13.848274     13.659233     13.652832   \n",
       "75%       18.297501     17.639688     16.154918     15.499474     14.900585   \n",
       "max       66.268756     66.268196     66.264158     66.258487     66.258177   \n",
       "\n",
       "           ...               1267          1268          1269          1270  \\\n",
       "count      ...       16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       ...           0.000134      0.000133      0.003879      0.000131   \n",
       "std        ...           0.002728      0.002705      0.043869      0.002676   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           0.062225      0.061999      0.500000      0.061534   \n",
       "\n",
       "               1271          1272          1273          1274    pubchem_id  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       0.000129      0.002155      0.000127      0.001201  33107.484300   \n",
       "std        0.002633      0.032755      0.002594      0.024472  23456.785147   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000  12298.250000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000  27731.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000  55020.750000   \n",
       "max        0.059760      0.500000      0.057834      0.500000  74980.000000   \n",
       "\n",
       "                Eat  \n",
       "count  16242.000000  \n",
       "mean     -11.178969  \n",
       "std        3.659133  \n",
       "min      -23.245373  \n",
       "25%      -13.475805  \n",
       "50%      -10.835211  \n",
       "75%       -8.623903  \n",
       "max       -0.789513  \n",
       "\n",
       "[8 rows x 1278 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos = pd.read_csv(\"roboBohr.csv\")\n",
    "print(\"datos.shape:\",datos.shape)\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.817765</td>\n",
       "      <td>12.469551</td>\n",
       "      <td>12.458130</td>\n",
       "      <td>12.454607</td>\n",
       "      <td>12.447345</td>\n",
       "      <td>12.433065</td>\n",
       "      <td>12.426926</td>\n",
       "      <td>12.387474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25004</td>\n",
       "      <td>-19.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.649126</td>\n",
       "      <td>18.527789</td>\n",
       "      <td>17.891535</td>\n",
       "      <td>17.887995</td>\n",
       "      <td>17.871731</td>\n",
       "      <td>17.852586</td>\n",
       "      <td>17.729842</td>\n",
       "      <td>15.864270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25005</td>\n",
       "      <td>-10.161019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.830377</td>\n",
       "      <td>12.512263</td>\n",
       "      <td>12.404775</td>\n",
       "      <td>12.394493</td>\n",
       "      <td>12.391564</td>\n",
       "      <td>12.324461</td>\n",
       "      <td>12.238106</td>\n",
       "      <td>10.423249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25006</td>\n",
       "      <td>-9.376619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.875810</td>\n",
       "      <td>17.871259</td>\n",
       "      <td>17.862402</td>\n",
       "      <td>17.850920</td>\n",
       "      <td>17.850440</td>\n",
       "      <td>12.558105</td>\n",
       "      <td>12.557645</td>\n",
       "      <td>12.517583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25009</td>\n",
       "      <td>-13.776438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.883818</td>\n",
       "      <td>17.868256</td>\n",
       "      <td>17.864221</td>\n",
       "      <td>17.818540</td>\n",
       "      <td>12.508657</td>\n",
       "      <td>12.490519</td>\n",
       "      <td>12.450098</td>\n",
       "      <td>10.597068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25011</td>\n",
       "      <td>-8.537140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          0          1          2          3          4  \\\n",
       "0           0  73.516695  17.817765  12.469551  12.458130  12.454607   \n",
       "1           1  73.516695  20.649126  18.527789  17.891535  17.887995   \n",
       "2           2  73.516695  17.830377  12.512263  12.404775  12.394493   \n",
       "3           3  73.516695  17.875810  17.871259  17.862402  17.850920   \n",
       "4           4  73.516695  17.883818  17.868256  17.864221  17.818540   \n",
       "\n",
       "           5          6          7          8    ...      1267  1268  1269  \\\n",
       "0  12.447345  12.433065  12.426926  12.387474    ...       0.0   0.0   0.5   \n",
       "1  17.871731  17.852586  17.729842  15.864270    ...       0.0   0.0   0.0   \n",
       "2  12.391564  12.324461  12.238106  10.423249    ...       0.0   0.0   0.0   \n",
       "3  17.850440  12.558105  12.557645  12.517583    ...       0.0   0.0   0.0   \n",
       "4  12.508657  12.490519  12.450098  10.597068    ...       0.0   0.0   0.0   \n",
       "\n",
       "   1270  1271  1272  1273  1274  pubchem_id        Eat  \n",
       "0   0.0   0.0   0.0   0.0   0.0       25004 -19.013763  \n",
       "1   0.0   0.0   0.0   0.0   0.0       25005 -10.161019  \n",
       "2   0.0   0.0   0.0   0.0   0.0       25006  -9.376619  \n",
       "3   0.0   0.0   0.0   0.0   0.0       25009 -13.776438  \n",
       "4   0.0   0.0   0.0   0.0   0.0       25011  -8.537140  \n",
       "\n",
       "[5 rows x 1278 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total = len(datos)\n",
    "df_trai = datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_vali = datos[int(0.6*total):int(0.85*total)]        #25% de los datos\n",
    "df_test = datos[int(0.85*total)::]                     #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `Unnamed: 0` representa el id del compuesto dentro del dataset, y `pubchem_id` parece ser un id general para identificar el compuesto. Ambas columnas se remueven porque la asignación de estos índices es arbitraria y su valor no debería estar relacionado con el resultado que debe entregar nuestro modelo (la Entalpía de Atomización, correspondiente a la columna `Eat`). Aunque el modelo debería detectar que no hay correlación entre `Eat` y estos atributos, es mejor removerlos para no *confundir* el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "a.1) Una buena práctica es la de normalizar los datos antes de trabajar con el modelo. **Explique por qué se aconseja dicho preprocesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trai_scaled (9745, 1275)\n",
      "X_vali_scaled (4060, 1275)\n",
      "X_test_scaled (2437, 1275)\n"
     ]
    }
   ],
   "source": [
    "# Get scaler and scale data\n",
    "scaler = StandardScaler().fit(df_trai)\n",
    "X_trai_scaled = pd.DataFrame(scaler.transform(df_trai),columns=df_trai.columns)\n",
    "X_vali_scaled = pd.DataFrame(scaler.transform(df_vali),columns=df_vali.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "# Get targets\n",
    "y_trai = df_trai.pop('Eat').values.reshape(-1,1)\n",
    "y_vali = df_vali.pop('Eat').values.reshape(-1,1)\n",
    "y_test = df_test.pop('Eat').values.reshape(-1,1)\n",
    "# Remove targets from attributes\n",
    "X_trai_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_vali_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "# \n",
    "print(\"X_trai_scaled\",X_trai_scaled.shape)\n",
    "print(\"X_vali_scaled\",X_vali_scaled.shape)\n",
    "print(\"X_test_scaled\",X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos modelos de aprendizaje son suceptibles a la escala de los atributos, por ese motivo, atributos con órdenes de magnitud mayores pueden afectar más a los mismos.\n",
    "Para hacer el aprendizaje **independiente** de las unidades de medición en que se presentan estos atributos y evitar un **bias** del aprendizaje hacia unos por sobre otros, es que se normalizan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "source": [
    "---\n",
    "b) Muestre en un gráfico el error cuadrático (MSE) para el conjunto de entrenamiento y de pruebas vs número de *epochs* de entrenamiento, para una red *feedforward* de 3 capas, con 256 unidades ocultas y función de activación sigmoidal. Entrene la red usando gradiente descendente estocástico con tasa de aprendizaje (learning rate) 0.01 y 250 epochs de entrenamiento, en el conjunto de entrenamiento y de validación. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create model:\n",
    "def create_model(activation='sigmoid',initializer='uniform',\n",
    "                 lr=None,decay=None,optimizer=None,\n",
    "                 layer1_reg=None,layer2_reg=None,dropout=None,\n",
    "                 neurons=256):\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons,input_dim=X_trai_scaled.shape[1],\n",
    "                    kernel_initializer=initializer,activation=activation,\n",
    "                    kernel_regularizer=layer1_reg))\n",
    "    if dropout:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, kernel_initializer=initializer,activation=\"linear\",\n",
    "                    kernel_regularizer=layer2_reg))\n",
    "    # Set optimizer\n",
    "    if optimizer is None:\n",
    "        # default values\n",
    "        if lr is None:\n",
    "            if activation==\"sigmoid\": lr = 0.01\n",
    "            elif activation==\"relu\" : lr = 0.001\n",
    "        if decay is None: decay = 0.0\n",
    "        optimizer = SGD(lr=lr,decay=decay)\n",
    "    else:\n",
    "        assert lr is None and decay is None\n",
    "    # Compile model with optimizer\n",
    "    model.compile(optimizer=optimizer,loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradual_color(i,total):\n",
    "    if i<total//2:\n",
    "        return (2.0*i/float(total),0.0,0.0)\n",
    "    else:\n",
    "        return (1.0,2.0*(i-total//2)/float(total),0.0)\n",
    "\n",
    "class PlotErrors(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,text=None,labels=None,gradual=False,save_name=None):\n",
    "        self.trai_err = []\n",
    "        self.vali_err = []\n",
    "        self.save_name = save_name\n",
    "        self.loaded = False\n",
    "        # Load cache\n",
    "        if os.path.isfile(self.save_name):\n",
    "            print(\"Loaded from %s\"%save_name)\n",
    "            data = np.load(self.save_name)\n",
    "            print(\"data.shape\",data.shape)\n",
    "            self.trai_err = data[0]\n",
    "            self.vali_err = data[1]\n",
    "            self.loaded = True\n",
    "        # Labels\n",
    "        self.labels = labels\n",
    "        self.gradual = gradual\n",
    "        self.trai_title = \"Training error v/s epoch\"\n",
    "        self.vali_title = \"Validaton error v/s epoch\"\n",
    "        if text:\n",
    "            self.trai_title = self.trai_title+\" for different \"+text\n",
    "            self.vali_title = self.vali_title+\" for different \"+text\n",
    "        self.widget = interact(self.display).widget\n",
    "\n",
    "    def display(self):\n",
    "        fig, ax = plt.subplots(1,2,figsize=(12,6),sharex=True,sharey=True)\n",
    "        ax[0].set_ylim(0.01,1000)\n",
    "        # Legend and colors:\n",
    "        legc = {}\n",
    "        # Train error:\n",
    "        ax[0].set(xlabel='epoch',ylabel='training error',title=self.trai_title)\n",
    "        for i in range(len(self.trai_err)):\n",
    "            #\n",
    "            if self.labels and self.gradual: legc['c'] = gradual_color(i,len(self.labels))\n",
    "            if self.labels: legc['label'] = self.labels[i]\n",
    "            #\n",
    "            x = np.arange(1,1+len(self.trai_err[i]))\n",
    "            ax[0].semilogy(x,self.trai_err[i],linewidth=2,**legc)\n",
    "        ax[0].grid()\n",
    "        if self.labels: ax[0].legend()\n",
    "        # Validation error\n",
    "        ax[1].set(xlabel='epoch',ylabel='validation error',title=self.vali_title)\n",
    "        for i in range(len(self.vali_err)):\n",
    "            #\n",
    "            if self.labels and self.gradual: legc['c'] = gradual_color(i,len(self.labels))\n",
    "            if self.labels: legc['label'] = self.labels[i]\n",
    "            #\n",
    "            x = np.arange(1,1+len(self.vali_err[i]))\n",
    "            ax[1].semilogy(x,self.vali_err[i],linewidth=2,**legc)\n",
    "        ax[1].grid()\n",
    "        if self.labels: ax[1].legend()\n",
    "        #\n",
    "        plt.show()\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        # Stop if loss is NaN\n",
    "        loss = logs.get('loss')\n",
    "        if np.isnan(loss):\n",
    "            self.model.stop_training = True\n",
    "            print(\"Loss is nan! Stop.\")\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.trai_err.append([])\n",
    "        self.vali_err.append([])\n",
    "    \n",
    "    def on_train_end(self,logs={}):\n",
    "        if self.save_name and (self.labels is None or len(self.vali_err)==len(self.labels)):\n",
    "            siz_x = max([len(x) for x in self.trai_err])\n",
    "            u = np.zeros((2,len(self.trai_err),siz_x))\n",
    "            u[:,:] = np.nan\n",
    "            for i in range(len(self.trai_err)):\n",
    "                u[0,i,:len(self.trai_err[i])] = self.trai_err[i]\n",
    "                u[1,i,:len(self.vali_err[i])] = self.vali_err[i]\n",
    "            np.save(self.save_name,u)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        vali_loss = logs.get('val_loss')\n",
    "        trai_loss = logs.get('loss')\n",
    "        self.vali_err[-1].append(vali_loss)\n",
    "        self.trai_err[-1].append(trai_loss)\n",
    "        self.widget.update()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250\n",
    "EXPERIMENTS = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1b.npy\n",
      "data.shape (2, 5, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6cd242fec54c67b1bb7bcf767339cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "callback = PlotErrors(\"experiments\",save_name=\"1b.npy\")\n",
    "if not callback.loaded:\n",
    "    for i in range(EXPERIMENTS):\n",
    "        model = create_model(activation=\"sigmoid\")\n",
    "        model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar como ambos errores disminuyen cada vez más lentamente, este comportamiento no difiere casi nada para la misma red inicializada con pesos diferentes. \n",
    "\n",
    "El error de testing no empeora pese a las 250 epoch, lo que sugiere que se puede entrenar más para obtener una mejor accuracy. \n",
    "\n",
    "Se puede ver que el error de validación varía bastante, lo que se puede deber a que el dataset es pequeño y que el conjunto de validación no participa en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "c) Repita el paso anterior, utilizado ’**ReLU**’ como función de activación y compare con lo obtenido en b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1c.npy\n",
      "data.shape (2, 5, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df9a1c998424c3681677fd7e0f58406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "callback = PlotErrors(\"experiments\",save_name=\"1c.npy\")\n",
    "if not callback.loaded:\n",
    "    for i in range(EXPERIMENTS):\n",
    "        initi = keras.initializers.RandomUniform(-1e-5,1e-5)\n",
    "        model = create_model(activation=\"relu\",initializer=initi)\n",
    "        model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "            validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "        del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convergencia es más **lenta** que con la activación **sigmoide**, además, se puede ver que se está generalizando mal, ya que el error de validación aumenta y es al menos un órden de magnitud mayor.\n",
    "\n",
    "No se nota **divergencia** en el error de entrenamiento, sin embargo, hay una diferencia significativa en en el **error de validación** para cada uno de los casos de prueba.\n",
    "\n",
    "Cabe destacar que fue necesario bajar la **tasa de aprendizaje** a 0.001 y utilizar una **inicialización lineal** con **valores pequeños** ya que de otra manera resultaban valores `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "d) Repita b) y c) variando la tasa de aprendizaje (*learning rate*) en un rango sensible. Comente. Si observara divergencia durante el entrenamiento, determine si esto ocurre para cada repetición del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05 ]\n",
      "[0.0005 0.001  0.0015 0.002  0.0025 0.003  0.0035 0.004  0.0045 0.005 ]\n"
     ]
    }
   ],
   "source": [
    "n_lr = 10\n",
    "learn_rate = {\"sigmoid\":np.arange(1,n_lr+1)/(20.0*n_lr),\"relu\":np.arange(1,n_lr+1)/(200.0*n_lr)}\n",
    "print(learn_rate[\"sigmoid\"])\n",
    "print(learn_rate[\"relu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1d_sigmoid.npy\n",
      "data.shape (2, 10, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d19292291444e290521d2ddbebb047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1d_relu.npy\n",
      "data.shape (2, 10, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd017250b21465bad6a683764a5999f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"learning rates\",[str(x) for x in learn_rate[activ]],gradual=True,\n",
    "                          save_name=\"1d_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for lr in learn_rate[activ]:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,lr=lr)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nótese que para **LR** grandes la pérdida se transformó en `NaN`, lo que terminó el entrenamiento.\n",
    "\n",
    "Para la activación **sigmoide** se puede notar que la **convergencia** es más rápida mientras **mayor** es la **tasa de aprendizaje**. Para **relu**, con **LR** de 0.003, se logró generalizar mejor, posiblemente porque se pudieron **evitar óptimos locales**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "e) Entrene los modelos considerados en b) y c) usando *progressive decay*. Compare y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-06 4.64158883e-06 2.15443469e-05 1.00000000e-04\n",
      " 4.64158883e-04 2.15443469e-03 1.00000000e-02 4.64158883e-02\n",
      " 2.15443469e-01 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 0.02\n",
    "n_decays = 10\n",
    "decays = np.logspace(-6,0,n_decays)\n",
    "print(decays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1e_sigmoid.npy\n",
      "data.shape (2, 10, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128a3a99bda94bce844dfb59627454b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1e_relu.npy\n",
      "data.shape (2, 10, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3a190b896242829a0c988c71b68a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"learning decays\",[str(x) for x in decays],gradual=True,\n",
    "                         save_name=\"1e_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for dec in decays:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,lr=INIT_LR,decay=dec)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que decays muy altos estancan el aprendizaje en cierto punto, si son suficientes permiten a la red converger más rápido y generalizar mejor (ver **sigmoide 0.0001**), sin embargo, si son muy pequeños no tienen un efecto significativo (**sigmoide 2.1544e-05**).\n",
    "\n",
    "Se colocó una learning rate relativamente alta pero no lo suficiente como para que resulten \n",
    "pérdidas `nan`.\n",
    "\n",
    "<!-- De acuerdo con [esta respuesta](https://stats.stackexchange.com/a/211340), la tasa de aprendizaje dado un *decay* $D$ es:\n",
    "$$\n",
    "L_t = L_0 \\, \\frac{1}{1 +  t \\cdot D}\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "f) Entrene los modelos considerados en b) y c) utilizando SGD en mini-*batches*. Experimente con diferentes tamaños del *batch*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2048 1024  512  256  128   64   32   16    8]\n"
     ]
    }
   ],
   "source": [
    "n_batches = 9\n",
    "batch_sizes = np.array(np.logspace(11,3,n_batches,base=2),dtype='int')\n",
    "print(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1f_sigmoid.npy\n",
      "data.shape (2, 9, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b917f0783bb477db8c7237890d9b0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1f_relu.npy\n",
      "data.shape (2, 9, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c7641a42d74c389d9a156d8962a658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"batch sizes\",[str(x) for x in batch_sizes],gradual=True,\n",
    "                         save_name=\"1f_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for bs in batch_sizes:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0,\n",
    "                validation_data=(X_vali_scaled, y_vali), callbacks=[callback],\n",
    "                batch_size=bs)\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que las redes con activación **sigmoide** siguen **generalizando mejor**, mientras más pequeños, los **batch sizes** dan mejores resultados, aunque demoraron más en entrenar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "g) Entrene los modelos obtenidos en b) y c) utilizando estrategias modernas para adaptar la tasa de aprendizaje. Compare los desempeños de adagrad, adadelta, RMSprop y adam. ¿Se observa en algún caso un mejor resultado final? ¿Se observa en algún caso una mayor velocidad de convergencia sobre el dataset de entrenamiento? ¿Sobre el dataset de validación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [SGD,Adam,RMSprop,Adagrad,Adadelta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1g_sigmoid.npy\n",
      "data.shape (2, 5, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e808f3d5d2254771bd5212698424e73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1g_relu.npy\n",
      "data.shape (2, 5, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee3640102d4477f928f601dbace58a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"optimizers\",[x.__name__ for x in optimizers],\n",
    "                         save_name=\"1g_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for opt in optimizers:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,optimizer=opt(lr=0.01))\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la activación **sigmoide** SGD obtuvo mejores resultados, en cambio para **relu** el mejor fue obtenido con **Adagrad**. **Adagrad** parece confiable para ambos casos.\n",
    "\n",
    "**RMSprop** y **Adam** varían bastante en el score de validación, en el caso de **relu** también lo hacen en el de entrenamiento y no son efectivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "h) Entrene los modelos obtenidos en b) y c) utilizando regularizadores $l_1$ y $l_2$ (*weight decay*). Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente. Además evalúe el efecto de regularizar solo la primera capa *vs* la segunda, comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [l1(0.001),l1(0.01),l1(0.1),l2(0.001),l2(0.01),l2(0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1h1_sigmoid.npy\n",
      "data.shape (2, 6, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a71c89a0d934d6f8bfe38596b135ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1h1_relu.npy\n",
      "data.shape (2, 6, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d0fe9fc354b31ab55bbc1c3478324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"regularizers\",[\"l1=%.3f,l2=%.3f \"%(x.l1,x.l2) for x in regs],\n",
    "                         save_name=\"1h1_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for reg in regs:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,\n",
    "                                 layer1_reg=reg,layer2_reg=reg)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **regularizadores** $l1=0.1$ y $l1=0.01$ son muy **altos** para ambos casos. Mejores resultados se obtienen con las regularizaciones más **bajas**, lo que hace pensar que se debió probar con valores aun más bajos, ya que están afectando mucho la **pérdida**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_regs = {\"l1_1layer\":(l1(0.01),None),\n",
    "             \"l1_2layer\":(None,l1(0.01)),\n",
    "             \"l1_both\"  :(l1(0.01),l1(0.01)),\n",
    "             \"l2_1layer\":(l2(0.01),None),\n",
    "             \"l2_2layer\":(None,l2(0.01)),\n",
    "             \"l2_both\"  :(l2(0.01),l2(0.01))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1h2_sigmoid.npy\n",
      "data.shape (2, 6, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17abd9887d4e47a28aa24d2c50c45465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1h2_relu.npy\n",
      "data.shape (2, 6, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5686f6375a9449b498d32eb9049f4a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = list(dual_regs.keys())\n",
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"regularizers\",names,\n",
    "                         save_name=\"1h2_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for nam in names:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,\n",
    "                                 layer1_reg=dual_regs[nam][0],layer2_reg=dual_regs[nam][1])\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores resultados se obtienen con regularizaciones **l2**, en ambos casos la activación **sigmoide** funciona mejor cuando se aplica en la **segunda capa**. En el caso de **relu**, cuando se aplica en ambas mejora más, lo que tiene sentido puesto que en la primera pueden surgir pesos grandes. \n",
    "\n",
    "Al menos en este ejemplo las **regularizaciones** no parecen afectar la **capacidad de generalización**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "i) Entrene los modelos obtenidos en b) y c) utilizando *Dropout*. Compare los desempeños de prueba obtenidos antes y después de regularizar. Experimente con distintos valores del parámetro de regularización y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropouts = [0.2,0.4,0.6,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1i_sigmoid.npy\n",
      "data.shape (2, 4, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a98ae4c70c407eab9da6eaaa7ee34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from 1i_relu.npy\n",
      "data.shape (2, 4, 250)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30b86d7da9942f9bafe1bdd26a08279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for activ in (\"sigmoid\",\"relu\"):\n",
    "    callback = PlotErrors(\"dropouts\",[str(x) for x in dropouts],\n",
    "                         save_name=\"1i_%s.npy\"%activ)\n",
    "    if not callback.loaded:\n",
    "        for drop in dropouts:\n",
    "            initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "            model = create_model(activation=activ,initializer=initi,dropout=drop)\n",
    "            model.fit(X_trai_scaled, y_trai, epochs=EPOCHS, verbose=0, batch_size=BATCH_SIZE,\n",
    "                        validation_data=(X_vali_scaled, y_vali), callbacks=[callback])\n",
    "            del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **dropout** muy **alto** genera pérdidas `NaN`. Mayores dropouts son efectivos para mejorar la capacidad de generalización en las **relu**, sin embargo, son malas en la **sigmoide**, que está obteniendo mejores resultados.\n",
    "\n",
    "Se pueden ver los efectos del **dropout** en el error de entrenamiento, y esto se puede usar para medir cuando se está eligiendo un **dropout**, muy alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "j) Fijando todos los demás hiper-parámetros del modelo definido en b) y en c), utilice validación cruzada con un número de *folds* igual a *K* = 5 y *K*=10 para determinar el mejor valor correspondiente a un parámetro que usted elija (tasa de aprendizaje, número de neuronas, parámetro de regularización, etc) ¿El mejor parámetro para la red con sigmoidal es distinto que para ReLU? ¿Porqué sucede? Además mida el error real del modelo sobre el conjunto de pruebas, compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_numbers = [64,128,192,256,320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "text",
    "id": "84-hf1CuNqhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation sigmoid:\n",
      "\tFolds 5:\n",
      "\t\t  64 Neurons:  mse_cv:   0.316276  mse_test:   0.226028\n",
      "\t\t 128 Neurons:  mse_cv:   0.325131  mse_test:   0.220594\n",
      "\t\t 192 Neurons:  mse_cv:   0.333575  mse_test:   0.176910\n",
      "\t\t 256 Neurons:  mse_cv:   0.329223  mse_test:   0.084800\n",
      "\t\t 320 Neurons:  mse_cv:   0.329049  mse_test:   0.183090\n",
      "\tFolds 10:\n",
      "\t\t  64 Neurons:  mse_cv:   0.339446  mse_test:   0.230289\n",
      "\t\t 128 Neurons:  mse_cv:   0.366717  mse_test:   0.074861\n",
      "\t\t 192 Neurons:  mse_cv:   0.542468  mse_test:   0.300941\n",
      "\t\t 256 Neurons:  mse_cv:   0.302596  mse_test:   0.254113\n",
      "\t\t 320 Neurons:  mse_cv:   0.408010  mse_test:   0.213780\n",
      "Activation relu:\n",
      "\tFolds 5:\n",
      "\t\t  64 Neurons:  mse_cv:   0.338098  mse_test:   0.176293\n",
      "\t\t 128 Neurons:  mse_cv:   0.310144  mse_test:   0.221150\n",
      "\t\t 192 Neurons:  mse_cv:   0.314514  mse_test:   0.194890\n",
      "\t\t 256 Neurons:  mse_cv:   0.307469  mse_test:   0.233891\n",
      "\t\t 320 Neurons:  mse_cv:   0.316362  mse_test:   0.178952\n",
      "\tFolds 10:\n",
      "\t\t  64 Neurons:  mse_cv:   0.908201  mse_test:   2.503101\n",
      "\t\t 128 Neurons:  mse_cv:   0.361111  mse_test:   0.170762\n",
      "\t\t 192 Neurons:  mse_cv:   0.269565  mse_test:   0.180801\n",
      "\t\t 256 Neurons:  mse_cv:   0.426513  mse_test:   0.385790\n",
      "\t\t 320 Neurons:  mse_cv:   0.377430  mse_test:   0.178350\n"
     ]
    }
   ],
   "source": [
    "recompute = False:\n",
    "    \n",
    "if recompute:\n",
    "    # Create the folds\n",
    "    for activation in (\"sigmoid\",\"relu\"):\n",
    "        print(\"Activation %s:\"%activation)\n",
    "        for folds in (5,10):\n",
    "            print(\"\\tFolds %d:\"%folds)\n",
    "            Xm = X_trai_scaled.values\n",
    "            ym = y_trai\n",
    "            for nneurons in neuron_numbers:\n",
    "                kfold = KFold(n_splits=folds,shuffle=False)\n",
    "                cvscores = []\n",
    "                for train, val in kfold.split(X_trai_scaled):\n",
    "                    initi = keras.initializers.RandomUniform(-1e-5,1e-5,seed=42)\n",
    "                    model = create_model(activation=activ,initializer=initi,neurons=nneurons)\n",
    "                    model.fit(Xm[train],ym[train], epochs=EPOCHS, verbose=0,\n",
    "                        batch_size=BATCH_SIZE)\n",
    "                    scores = model.evaluate(Xm[val], ym[val], verbose=0)\n",
    "                    cvscores.append(scores)\n",
    "                mse_cv = np.mean(cvscores)\n",
    "                mse_test = np.mean(model.evaluate(X_test_scaled,y_test, verbose=0))\n",
    "                print(\"\\t\\t%4d Neurons:  mse_cv: %10.6f  mse_test: %10.6f\"%(\n",
    "                    nneurons,mse_cv,mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se decidió utilizar el **número de neuronas** como hiper parámetro. Para el caso de activación **sigmoide** el menor error se obtuvo con redes de 256 neuronas (0.084800) y para **relu** con 192 neuronas (0.180801).\n",
    "\n",
    "La gran variación entre el **error CV de entrenamiento** el modelo entrenado para 5 Folds y su análogo de 10 Folds hacen ver que hay una variación importante producto de factores aleatorios, pudiendo ser la **elección del fold** (si los conjuntos son pequeños la variación del error puede ser alta), la **inicialización aleatoria**.\n",
    "\n",
    "Se puede ver que el error obtenido en la **cross validation** no es una buena representación del error obtenido en el conjunto de test, pudiendo deberse a las mismas razones anteriores. Sin embargo se puede ver que el error con **5 Folds** es mucho más estable, puesto que trata con conjuntos más grandes. Los resultados no son concluyentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Enunciado.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
